{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86b170f9",
   "metadata": {},
   "source": [
    "# TPT Scraper Configuration Update & Testing\n",
    "\n",
    "This notebook demonstrates the updated TPT scraper configuration with the new URL parameter structure and tests all stages for correctness.\n",
    "\n",
    "## New URL Structure\n",
    "The scraper now uses the following URL structure:\n",
    "```\n",
    "https://www.teacherspayteachers.com/browse/[resource-type]/[grade-level]/[subject]/[format]/[price]/[supports]?order=[sorting-method]\n",
    "```\n",
    "\n",
    "## Updated Configuration Parameters\n",
    "- **Resource Type**: teacher-tools, hands-on-activities, instruction, student-practice, etc.\n",
    "- **Grade Level**: elementary, middle-school, high-school with specific grades\n",
    "- **Subject**: social-emotional categories\n",
    "- **Format**: pdf, digital, video, audio, etc.\n",
    "- **Price Options**: free, under-5, 5-to-10, above-10, on-sale\n",
    "- **Supports**: special-education, speech-therapy\n",
    "- **Sorting Methods**: Relevance, Rating, Price-Asc, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e62f11b",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for testing the updated scraper configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fafa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiosqlite\n",
    "import aiohttp_client_cache\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the current directory to path to import tptscrape\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Import our scraper functions\n",
    "from tptscrape import (\n",
    "    setup_db,\n",
    "    extraction_test,\n",
    "    extraction_stage,\n",
    "    processing_stage,\n",
    "    processing_free_download_stage,\n",
    "    update_config_metadata,\n",
    "    check_config_metadata,\n",
    "    backfill_preview_keywords,\n",
    "    build_page_url,\n",
    "    RESOURCE_TYPES,\n",
    "    GRADE_LEVELS,\n",
    "    SUBJECTS,\n",
    "    FORMATS,\n",
    "    PRICE_OPTIONS,\n",
    "    SUPPORTS,\n",
    "    SORTING_METHODS\n",
    ")\n",
    "\n",
    "# Configure logging for better output\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d76bba",
   "metadata": {},
   "source": [
    "## 2. Load and Display Configuration\n",
    "\n",
    "Load the current configuration from config.json and display its contents to verify the settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8321ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display current configuration\n",
    "with open('config.json', 'r', encoding='utf-8') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"📋 Current Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for key, value in config.items():\n",
    "    if isinstance(value, list):\n",
    "        print(f\"\\n{key.upper()}: ({len(value)} items)\")\n",
    "        if len(value) <= 10:\n",
    "            for item in value:\n",
    "                print(f\"  - {item}\")\n",
    "        else:\n",
    "            for item in value[:5]:\n",
    "                print(f\"  - {item}\")\n",
    "            print(f\"  ... and {len(value) - 5} more items\")\n",
    "    else:\n",
    "        print(f\"\\n{key.upper()}: {value}\")\n",
    "\n",
    "# Calculate total combinations\n",
    "total_combinations = (\n",
    "    len(config.get(\"resource_type\", [])) *\n",
    "    len(config.get(\"grade_level\", [])) *\n",
    "    len(config.get(\"subject\", [])) *\n",
    "    len(config.get(\"format\", [])) *\n",
    "    len(config.get(\"price_options\", [])) *\n",
    "    len(config.get(\"supports\", [])) *\n",
    "    len(config.get(\"sorting_methods\", []))\n",
    ")\n",
    "\n",
    "print(f\"\\n🔢 Total parameter combinations: {total_combinations:,}\")\n",
    "print(f\"📄 Total pages per combination: {config.get('total_pages', 42)}\")\n",
    "print(f\"🌐 Total possible URL combinations: {total_combinations * config.get('total_pages', 42):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457d2d8e",
   "metadata": {},
   "source": [
    "## 3. Test URL Building with Current Configuration\n",
    "\n",
    "Test the URL building function with various parameter combinations to ensure the new structure works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e467ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test URL building with different parameter combinations\n",
    "print(\"🔗 Testing URL Building with Different Parameter Combinations:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_cases = [\n",
    "    {\n",
    "        \"name\": \"Basic social-emotional search\",\n",
    "        \"params\": (\"\", \"\", \"social-emotional\", \"\", \"\", \"\", \"Relevance\", 1)\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Teacher tools for elementary\",\n",
    "        \"params\": (\"teacher-tools\", \"elementary\", \"social-emotional\", \"pdf\", \"free\", \"\", \"Rating\", 1)\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Activities for middle school with special education support\",\n",
    "        \"params\": (\"hands-on-activities\", \"middle-school\", \"social-emotional/character-education\", \"digital\", \"under-5\", \"special-education\", \"Price-Asc\", 2)\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"High school instruction materials\",\n",
    "        \"params\": (\"instruction\", \"high-school/12th-grade\", \"social-emotional/social-emotional-learning\", \"video\", \"above-10\", \"\", \"Most-Recent\", 3)\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Student practice worksheets\",\n",
    "        \"params\": (\"student-practice/worksheets\", \"elementary/3rd-grade\", \"social-emotional\", \"pdf\", \"5-to-10\", \"speech-therapy\", \"Rating-Count\", 1)\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    name = test_case[\"name\"]\n",
    "    resource_type, grade_level, subject, format_type, price_option, supports, sort_order, page = test_case[\"params\"]\n",
    "    \n",
    "    url = build_page_url(resource_type, grade_level, subject, format_type, price_option, supports, sort_order, page)\n",
    "    \n",
    "    print(f\"\\n{i}. {name}:\")\n",
    "    print(f\"   Parameters: {test_case['params']}\")\n",
    "    print(f\"   URL: {url}\")\n",
    "\n",
    "print(\"\\n✅ URL building tests completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1561736",
   "metadata": {},
   "source": [
    "## 4. Run Database Setup\n",
    "\n",
    "Initialize the SQLite database and create all necessary tables with the updated schema for the new URL parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ec20cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run database setup\n",
    "print(\"🗄️ Setting up SQLite database with updated schema...\")\n",
    "\n",
    "# Run the database setup\n",
    "await setup_db()\n",
    "\n",
    "print(\"✅ Database setup completed successfully!\")\n",
    "\n",
    "# Check if database file exists and show its size\n",
    "db_path = Path(\"scrape_cache.db\")\n",
    "if db_path.exists():\n",
    "    size_mb = db_path.stat().st_size / (1024 * 1024)\n",
    "    print(f\"📊 Database file size: {size_mb:.2f} MB\")\n",
    "    \n",
    "    # Check table structure\n",
    "    async with aiosqlite.connect(\"scrape_cache.db\") as db:\n",
    "        # Check extracted_urls table structure\n",
    "        async with db.execute(\"PRAGMA table_info(extracted_urls)\") as cursor:\n",
    "            columns = await cursor.fetchall()\n",
    "            print(f\"\\n📋 extracted_urls table columns ({len(columns)} total):\")\n",
    "            for col in columns:\n",
    "                print(f\"  - {col[1]} ({col[2]})\")\n",
    "        \n",
    "        # Check product_data table structure  \n",
    "        async with db.execute(\"PRAGMA table_info(product_data)\") as cursor:\n",
    "            columns = await cursor.fetchall()\n",
    "            print(f\"\\n📋 product_data table columns ({len(columns)} total):\")\n",
    "            for col in columns:\n",
    "                print(f\"  - {col[1]} ({col[2]})\")\n",
    "else:\n",
    "    print(\"❌ Database file not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aeb87c",
   "metadata": {},
   "source": [
    "## 5. Test Extraction with Current Configuration\n",
    "\n",
    "Run a test extraction to fetch and store a sample of product URLs using the current configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125d6c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test extraction with current configuration\n",
    "print(\"🔍 Running extraction test with current configuration...\")\n",
    "\n",
    "# Run the extraction test\n",
    "await extraction_test(test_limit=5)\n",
    "\n",
    "print(\"\\n📊 Checking extraction results...\")\n",
    "\n",
    "# Check what was extracted\n",
    "async with aiosqlite.connect(\"scrape_cache.db\") as db:\n",
    "    # Count extracted URLs\n",
    "    async with db.execute(\"SELECT COUNT(*) FROM extracted_urls\") as cursor:\n",
    "        url_count = await cursor.fetchone()[0]\n",
    "    \n",
    "    # Count extracted pages\n",
    "    async with db.execute(\"SELECT COUNT(*) FROM extracted_pages\") as cursor:\n",
    "        page_count = await cursor.fetchone()[0]\n",
    "    \n",
    "    print(f\"📈 Extracted URLs: {url_count}\")\n",
    "    print(f\"📄 Extracted pages: {page_count}\")\n",
    "    \n",
    "    if url_count > 0:\n",
    "        # Show sample of extracted URLs with their parameters\n",
    "        async with db.execute(\"\"\"\n",
    "            SELECT url, resource_type, grade_level, subject, format, price_option, supports, sort_order, page \n",
    "            FROM extracted_urls \n",
    "            LIMIT 3\n",
    "        \"\"\") as cursor:\n",
    "            rows = await cursor.fetchall()\n",
    "            \n",
    "        print(f\"\\n📝 Sample extracted URLs:\")\n",
    "        for i, row in enumerate(rows, 1):\n",
    "            url, resource_type, grade_level, subject, format_type, price_option, supports, sort_order, page = row\n",
    "            print(f\"{i}. {url}\")\n",
    "            print(f\"   Parameters: resource_type='{resource_type}', grade_level='{grade_level}', subject='{subject}'\")\n",
    "            print(f\"               format='{format_type}', price='{price_option}', supports='{supports}'\")\n",
    "            print(f\"               sort='{sort_order}', page={page}\")\n",
    "\n",
    "print(\"\\n✅ Extraction test completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ced90d1",
   "metadata": {},
   "source": [
    "## 6. Configuration Statistics & Analysis\n",
    "\n",
    "Analyze the configuration parameters and their impact on scraping scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b4819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration statistics and analysis\n",
    "print(\"📊 Configuration Statistics & Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Display parameter counts\n",
    "param_stats = {\n",
    "    \"Resource Types\": len(RESOURCE_TYPES),\n",
    "    \"Grade Levels\": len(GRADE_LEVELS), \n",
    "    \"Subjects\": len(SUBJECTS),\n",
    "    \"Formats\": len(FORMATS),\n",
    "    \"Price Options\": len(PRICE_OPTIONS),\n",
    "    \"Supports\": len(SUPPORTS),\n",
    "    \"Sorting Methods\": len(SORTING_METHODS)\n",
    "}\n",
    "\n",
    "for param, count in param_stats.items():\n",
    "    print(f\"{param}: {count} options\")\n",
    "\n",
    "# Calculate scope\n",
    "base_combinations = 1\n",
    "for count in param_stats.values():\n",
    "    base_combinations *= count\n",
    "\n",
    "total_pages = config.get('total_pages', 42)\n",
    "total_urls = base_combinations * total_pages\n",
    "\n",
    "print(f\"\\n🔢 Calculation:\")\n",
    "print(f\"Base parameter combinations: {base_combinations:,}\")\n",
    "print(f\"Pages per combination: {total_pages}\")\n",
    "print(f\"Total possible URLs to scrape: {total_urls:,}\")\n",
    "\n",
    "# Estimate time and resources\n",
    "urls_per_minute = 60  # Conservative estimate\n",
    "total_minutes = total_urls / urls_per_minute\n",
    "total_hours = total_minutes / 60\n",
    "total_days = total_hours / 24\n",
    "\n",
    "print(f\"\\n⏱️ Time Estimates (at {urls_per_minute} URLs/minute):\")\n",
    "print(f\"Total minutes: {total_minutes:,.0f}\")\n",
    "print(f\"Total hours: {total_hours:,.1f}\")\n",
    "print(f\"Total days: {total_days:.1f}\")\n",
    "\n",
    "# Show some example parameter combinations\n",
    "print(f\"\\n🎯 Sample Parameter Combinations:\")\n",
    "import itertools\n",
    "\n",
    "# Get first few combinations\n",
    "sample_combinations = list(itertools.product(\n",
    "    RESOURCE_TYPES[:2], \n",
    "    GRADE_LEVELS[:2], \n",
    "    SUBJECTS[:2], \n",
    "    FORMATS[:2], \n",
    "    PRICE_OPTIONS[:2], \n",
    "    SUPPORTS[:2], \n",
    "    SORTING_METHODS[:2]\n",
    "))[:5]\n",
    "\n",
    "for i, combo in enumerate(sample_combinations, 1):\n",
    "    resource_type, grade_level, subject, format_type, price_option, supports, sort_order = combo\n",
    "    url = build_page_url(resource_type, grade_level, subject, format_type, price_option, supports, sort_order, 1)\n",
    "    print(f\"{i}. {url}\")\n",
    "\n",
    "print(f\"\\n✅ Configuration analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfee4aa6",
   "metadata": {},
   "source": [
    "## 7. Test Processing Stage (Small Scale)\n",
    "\n",
    "Test the processing stage by scraping product data for the extracted URLs to ensure the new parameter structure works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53896ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test processing stage on a small scale\n",
    "print(\"⚙️ Testing processing stage with extracted URLs...\")\n",
    "\n",
    "# Check how many URLs we have to process\n",
    "async with aiosqlite.connect(\"scrape_cache.db\") as db:\n",
    "    async with db.execute(\"SELECT COUNT(*) FROM extracted_urls\") as cursor:\n",
    "        total_urls = await cursor.fetchone()[0]\n",
    "    \n",
    "    async with db.execute(\"SELECT COUNT(*) FROM product_data\") as cursor:\n",
    "        processed_urls = await cursor.fetchone()[0]\n",
    "\n",
    "print(f\"📈 URLs to process: {total_urls}\")\n",
    "print(f\"📊 Already processed: {processed_urls}\")\n",
    "print(f\"🔄 Remaining: {total_urls - processed_urls}\")\n",
    "\n",
    "if total_urls > 0:\n",
    "    print(f\"\\n🚀 Running processing stage (batch size: 5)...\")\n",
    "    \n",
    "    # Import the processing function with smaller batch size for testing\n",
    "    from tptscrape import scrape_product_data\n",
    "    import aiohttp_client_cache\n",
    "    \n",
    "    # Get a few URLs to process\n",
    "    async with aiosqlite.connect(\"scrape_cache.db\") as db:\n",
    "        async with db.execute(\"\"\"\n",
    "            SELECT url, resource_type, grade_level, subject, format, price_option, supports, sort_order, page \n",
    "            FROM extracted_urls \n",
    "            WHERE url NOT IN (SELECT url FROM product_data WHERE url IS NOT NULL)\n",
    "            LIMIT 3\n",
    "        \"\"\") as cursor:\n",
    "            urls_to_process = await cursor.fetchall()\n",
    "    \n",
    "    if urls_to_process:\n",
    "        # Process these URLs manually to test the new structure\n",
    "        async with aiohttp_client_cache.CachedSession(cache_name=\"test_cache\", expire_after=3600) as session:\n",
    "            for i, record in enumerate(urls_to_process, 1):\n",
    "                url, resource_type, grade_level, subject, format_type, price_option, supports, sort_order, page = record\n",
    "                \n",
    "                print(f\"\\n🔍 Processing URL {i}/{len(urls_to_process)}: {url}\")\n",
    "                print(f\"   Parameters: resource_type='{resource_type}', grade_level='{grade_level}', subject='{subject}'\")\n",
    "                \n",
    "                try:\n",
    "                    result = await scrape_product_data(session, url)\n",
    "                    if result:\n",
    "                        title, short_description, long_description, rating_value, number_of_ratings, product_price, preview_keywords, url = result\n",
    "                        \n",
    "                        # Build config metadata\n",
    "                        config_metadata = json.dumps({\n",
    "                            \"resource_type\": resource_type,\n",
    "                            \"grade_level\": grade_level,\n",
    "                            \"subject\": subject,\n",
    "                            \"format\": format_type,\n",
    "                            \"price_option\": price_option,\n",
    "                            \"supports\": supports,\n",
    "                            \"sort_order\": sort_order,\n",
    "                            \"page\": page\n",
    "                        })\n",
    "                        \n",
    "                        # Insert into database\n",
    "                        async with aiosqlite.connect(\"scrape_cache.db\") as db:\n",
    "                            await db.execute(\"\"\"\n",
    "                                INSERT OR IGNORE INTO product_data \n",
    "                                (title, short_description, long_description, rating_value, number_of_ratings, product_price, preview_keywords, url, resource_type, grade_level, subject, format, price_option, supports, sort_order, page, config_metadata)\n",
    "                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                            \"\"\", (title, short_description, long_description, rating_value, number_of_ratings, product_price, preview_keywords, url, resource_type, grade_level, subject, format_type, price_option, supports, sort_order, page, config_metadata))\n",
    "                            await db.commit()\n",
    "                        \n",
    "                        print(f\"   ✅ Title: {title}\")\n",
    "                        print(f\"   💰 Price: {product_price}\")\n",
    "                        print(f\"   ⭐ Rating: {rating_value} ({number_of_ratings} ratings)\")\n",
    "                        \n",
    "                    else:\n",
    "                        print(f\"   ❌ Failed to extract data\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"   ❌ Error: {e}\")\n",
    "        \n",
    "        # Check final results\n",
    "        async with aiosqlite.connect(\"scrape_cache.db\") as db:\n",
    "            async with db.execute(\"SELECT COUNT(*) FROM product_data\") as cursor:\n",
    "                final_count = await cursor.fetchone()[0]\n",
    "        \n",
    "        print(f\"\\n📊 Final processed count: {final_count}\")\n",
    "        print(\"✅ Processing stage test completed!\")\n",
    "    else:\n",
    "        print(\"ℹ️ No unprocessed URLs found. All URLs have already been processed.\")\n",
    "else:\n",
    "    print(\"⚠️ No URLs found to process. Run extraction first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab75c3b",
   "metadata": {},
   "source": [
    "## 8. Check and Update Configuration Metadata\n",
    "\n",
    "Verify that the config_metadata field is properly set for all product_data records with the new parameter structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb788a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and update configuration metadata\n",
    "print(\"🔧 Checking and updating configuration metadata...\")\n",
    "\n",
    "# Check current state of config_metadata\n",
    "async with aiosqlite.connect(\"scrape_cache.db\") as db:\n",
    "    async with db.execute(\"SELECT COUNT(*) FROM product_data\") as cursor:\n",
    "        total_products = await cursor.fetchone()[0]\n",
    "    \n",
    "    async with db.execute(\"SELECT COUNT(*) FROM product_data WHERE config_metadata IS NOT NULL\") as cursor:\n",
    "        with_metadata = await cursor.fetchone()[0]\n",
    "    \n",
    "    async with db.execute(\"SELECT COUNT(*) FROM product_data WHERE config_metadata IS NULL\") as cursor:\n",
    "        without_metadata = await cursor.fetchone()[0]\n",
    "\n",
    "print(f\"📊 Product data records:\")\n",
    "print(f\"   Total: {total_products}\")\n",
    "print(f\"   With metadata: {with_metadata}\")\n",
    "print(f\"   Without metadata: {without_metadata}\")\n",
    "\n",
    "if without_metadata > 0:\n",
    "    print(f\"\\n🔄 Updating metadata for {without_metadata} records...\")\n",
    "    await update_config_metadata()\n",
    "    \n",
    "    # Check again after update\n",
    "    async with aiosqlite.connect(\"scrape_cache.db\") as db:\n",
    "        async with db.execute(\"SELECT COUNT(*) FROM product_data WHERE config_metadata IS NOT NULL\") as cursor:\n",
    "            updated_count = await cursor.fetchone()[0]\n",
    "    \n",
    "    print(f\"✅ Updated metadata count: {updated_count}\")\n",
    "\n",
    "# Display sample records with metadata\n",
    "print(f\"\\n📋 Sample records with config_metadata:\")\n",
    "await check_config_metadata()\n",
    "\n",
    "# Show a parsed example of the metadata\n",
    "if total_products > 0:\n",
    "    async with aiosqlite.connect(\"scrape_cache.db\") as db:\n",
    "        async with db.execute(\"\"\"\n",
    "            SELECT config_metadata \n",
    "            FROM product_data \n",
    "            WHERE config_metadata IS NOT NULL \n",
    "            LIMIT 1\n",
    "        \"\"\") as cursor:\n",
    "            sample = await cursor.fetchone()\n",
    "    \n",
    "    if sample and sample[0]:\n",
    "        metadata = json.loads(sample[0])\n",
    "        print(f\"\\n🔍 Example config_metadata structure:\")\n",
    "        for key, value in metadata.items():\n",
    "            print(f\"   {key}: '{value}'\")\n",
    "\n",
    "print(\"\\n✅ Configuration metadata check completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b9676e",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "Summary of the configuration update and testing results, along with recommendations for next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0379026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and recommendations\n",
    "print(\"📋 TPT Scraper Configuration Update - Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n✅ Completed Tasks:\")\n",
    "print(\"  1. ✓ Updated configuration structure with new URL parameters\")\n",
    "print(\"  2. ✓ Modified database schema to support new parameter structure\")\n",
    "print(\"  3. ✓ Updated URL building function for new TPT URL format\")\n",
    "print(\"  4. ✓ Tested extraction stage with new parameters\")\n",
    "print(\"  5. ✓ Tested processing stage with new database structure\")\n",
    "print(\"  6. ✓ Verified configuration metadata handling\")\n",
    "\n",
    "print(\"\\n🔧 Key Changes Made:\")\n",
    "print(\"  • Separated folder_structures into individual parameters:\")\n",
    "print(\"    - resource_type, grade_level, subject, format, price_options, supports\")\n",
    "print(\"  • Updated URL structure to match TPT's browse format\")\n",
    "print(\"  • Enhanced database schema with new parameter columns\")\n",
    "print(\"  • Improved config metadata tracking\")\n",
    "\n",
    "print(\"\\n📊 Configuration Statistics:\")\n",
    "print(f\"  • Resource Types: {len(RESOURCE_TYPES)} options\")\n",
    "print(f\"  • Grade Levels: {len(GRADE_LEVELS)} options\")\n",
    "print(f\"  • Subjects: {len(SUBJECTS)} options\")\n",
    "print(f\"  • Formats: {len(FORMATS)} options\")\n",
    "print(f\"  • Price Options: {len(PRICE_OPTIONS)} options\")\n",
    "print(f\"  • Support Options: {len(SUPPORTS)} options\")\n",
    "print(f\"  • Sorting Methods: {len(SORTING_METHODS)} options\")\n",
    "\n",
    "total_combinations = len(RESOURCE_TYPES) * len(GRADE_LEVELS) * len(SUBJECTS) * len(FORMATS) * len(PRICE_OPTIONS) * len(SUPPORTS) * len(SORTING_METHODS)\n",
    "print(f\"  • Total parameter combinations: {total_combinations:,}\")\n",
    "\n",
    "print(\"\\n🚀 Next Steps:\")\n",
    "print(\"  1. Run full extraction stage: python tptscrape.py extract\")\n",
    "print(\"  2. Run processing stage: python tptscrape.py process\")\n",
    "print(\"  3. Monitor progress and adjust batch sizes if needed\")\n",
    "print(\"  4. Consider implementing additional filters for efficiency\")\n",
    "print(\"  5. Update any analysis scripts to use new parameter structure\")\n",
    "\n",
    "print(\"\\n⚠️ Important Notes:\")\n",
    "print(\"  • The new configuration generates significantly more URL combinations\")\n",
    "print(\"  • Consider running smaller batches initially to test performance\")\n",
    "print(\"  • Monitor TPT's rate limiting and adjust delays accordingly\")\n",
    "print(\"  • The improved price extraction should capture more accurate data\")\n",
    "\n",
    "print(\"\\n🎯 Ready to proceed with full-scale scraping using the updated configuration!\")\n",
    "\n",
    "# Show command examples\n",
    "print(\"\\n💻 Command Examples:\")\n",
    "print(\"  # Test extraction (small scale)\")\n",
    "print(\"  python tptscrape.py test\")\n",
    "print(\"\")\n",
    "print(\"  # Full extraction (all parameter combinations)\")  \n",
    "print(\"  python tptscrape.py extract\")\n",
    "print(\"\")\n",
    "print(\"  # Process extracted URLs\")\n",
    "print(\"  python tptscrape.py process\")\n",
    "print(\"\")\n",
    "print(\"  # Update metadata for existing records\")\n",
    "print(\"  python tptscrape.py update\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
